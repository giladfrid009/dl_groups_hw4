{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Parameter-Sharing Scheme for 2D Linear Equivariant Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "M = 8\n",
    "N = 7\n",
    "\n",
    "W = torch.zeros(M, N, M, N)\n",
    "A = torch.randn(M, N)\n",
    "\n",
    "for i1 in range(M):\n",
    "    for j1 in range(N):\n",
    "        for i2 in range(M):\n",
    "            for j2 in range(N):\n",
    "                if i1 == i2 and j1 == j2:\n",
    "                    W[i1, j1, i2, j2] = 1\n",
    "                elif i1 == i2 and j1 != j2:\n",
    "                    W[i1, j1, i2, j2] = 2\n",
    "                elif i1 != i2 and j1 == j2:\n",
    "                    W[i1, j1, i2, j2] = 3\n",
    "                else:\n",
    "                    W[i1, j1, i2, j2] = 4\n",
    "\n",
    "plt.imshow(W.reshape(M * N, M * N).to(torch.int32), cmap=\"tab10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Parameter-Sharing Scheme for 3D Linear Equivariant Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0e657057d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "M = 4\n",
    "N = 3\n",
    "K = 5\n",
    "\n",
    "W = torch.zeros(M, N, K, M, N, K)\n",
    "A = torch.randn(M, N, K)\n",
    "\n",
    "for i1 in range(M):\n",
    "    for j1 in range(N):\n",
    "        for p1 in range(K):\n",
    "            for i2 in range(M):\n",
    "                for j2 in range(N):\n",
    "                    for p2 in range(K):\n",
    "                        if i1 == i2 and j1 == j2 and p1 == p2:\n",
    "                            W[i1, j1, p1, i2, j2, p2] = 1\n",
    "                        if i1 == i2 and j1 == j2 and p1 != p2:\n",
    "                            W[i1, j1, p1, i2, j2, p2] = 2\n",
    "                        if i1 == i2 and j1 != j2 and p1 == p2:\n",
    "                            W[i1, j1, p1, i2, j2, p2] = 3\n",
    "                        if i1 == i2 and j1 != j2 and p1 != p2:\n",
    "                            W[i1, j1, p1, i2, j2, p2] = 4\n",
    "                        if i1 != i2 and j1 == j2 and p1 == p2:\n",
    "                            W[i1, j1, p1, i2, j2, p2] = 5\n",
    "                        if i1 != i2 and j1 == j2 and p1 != p2:\n",
    "                            W[i1, j1, p1, i2, j2, p2] = 6\n",
    "                        if i1 != i2 and j1 != j2 and p1 == p2:\n",
    "                            W[i1, j1, p1, i2, j2, p2] = 7\n",
    "                        if i1 != i2 and j1 != j2 and p1 != p2:\n",
    "                            W[i1, j1, p1, i2, j2, p2] = 0\n",
    "\n",
    "plt.imshow(W.reshape(M * N * K, M * N * K).to(torch.int32), cmap=\"tab10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following models were analyzed throughout this experiment:\n",
    "\n",
    "* *canonical-mlp* - MLP architecture with canonized-inputs        \n",
    "* *canonical-attn* - Transformer encoder with canonized-inputs       \n",
    "* *symmetry-mlp* - MLP architecture applied to all input permutations         \n",
    "* *symmetry-attn* - Transformer encoder applied to all input permutations        \n",
    "* *symmetry-sampling-mlp* - MLP architecture applied to 10 random input permutations\n",
    "* *symmetry-sampling-attn* - Transformer encoder applied to 10 random input permutations \n",
    "* *intrinsic* - Intrinsic invariant architecture made from equivariant and invariant linear layers            \n",
    "* *augmented-mlp* - MLP architecture with random permutation applied to the inputs during training       \n",
    "* *augmented-attn* - Transformer encoder with random permutation applied to the inputs during training       \n",
    "\n",
    "It's important to note that the symmetry model (without sub-sampling) are not present in the plots since the number of different permutations for $N=10$ is $N!=3628800$, and the model is not able to perform even a single forward pass for this number of permutations. Even when sub-sampling 5% of the permutations, the number of permutations is still too high for the model to handle. As result, in the symmetry-sampling models we sampled 10 random permutations instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-Set 100 ; N = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](images/train100_seq10_train.png)\n",
    "\n",
    "![text](images/train100_seq10_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-Set 100 ; N = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](images/train100_seq100_train.png)\n",
    "\n",
    "![text](images/train100_seq100_train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-Set 1000 ; N = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](images/train1000_seq10_train.png)\n",
    "\n",
    "![text](images/train1000_seq10_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-Set 1000 ; N = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](images/train1000_seq100_train.png)\n",
    "\n",
    "![text](images/train1000_seq100_train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-Set 10000 ; N = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](images/train10000_seq10_train.png)\n",
    "\n",
    "![text](images/train10000_seq10_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-Set 10000 ; N = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](images/train10000_seq100_train.png)\n",
    "\n",
    "![text](images/train10000_seq100_train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Invariance:\n",
    "\n",
    "|          Model         | Invariant |\n",
    "|------------------------|:---------:|\n",
    "| canonical-mlp          | T         |\n",
    "| canonical-attn        | T         |\n",
    "| symmetry-mlp           | T         |\n",
    "| symmetry-attn          | T         |\n",
    "| symmetry-sampling-mlp  | F         |\n",
    "| symmetry-sampling-attn | F         |\n",
    "| intrinsic              | T         |\n",
    "| augmented-mlp          | F         |\n",
    "| augmented-attn         | F         |\n",
    "\n",
    "\n",
    "##### Performance Metrics:   \n",
    "\n",
    "The performance were measured on input batch of size (32, N, 5) and on a CPU.\n",
    "\n",
    "| Model                  | N = 10         | N = 10        | N = 100        | N = 100       |\n",
    "| ---------------------- | -------------- | ------------- | -------------- | ------------- |\n",
    "|                        | Inference (ms) | Training (ms) | Inference (ms) | Training (ms) |\n",
    "| canonical-mlp          | 0.12           | 0.36          | 0.28           | 0.54          |\n",
    "| canonical-attn        | 0.86           | 4.47          | 10.77          | 46.38         |\n",
    "| symmetry-mlp           | OOT            | OOT           | OOT            | OOT           |\n",
    "| symmetry-attn          | OOT            | OOT           | OOT            | OOT           |\n",
    "| symmetry-sampling-mlp  | 0.6            | 0.95          | 0.99           | 1.56          |\n",
    "| symmetry-sampling-attn | 11.21          | 46.38         | 110.78         | 482.51        |\n",
    "| intrinsic              | 0.27           | 0.94          | 1.01           | 2.36          |\n",
    "| augmented-mlp          | 0.08           | 0.44          | 0.12           | 0.54          |\n",
    "| augmented-attn         | 0.85           | 4.68          | 11.46          | 48.58         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Challenges encountered during Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numeric Errors:\n",
    "\n",
    "Originally, we constructed the invariant network as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "def create_invariant_model(n: int, d: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        LinearEquivariant(in_channels=d, out_channels=10),\n",
    "        nn.ReLU(),\n",
    "        LinearEquivariant(in_channels=10, out_channels=10),\n",
    "        nn.ReLU(),\n",
    "        LinearInvariant(in_channels=10, out_channels=1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "```\n",
    "\n",
    "The problem that arises from this architecture is that the output range of the LinearInvariant is quite large, very often producing large numbers (i.e. 1000).\n",
    "As result, the sigmoid activation function would saturate, and return binary results (0 or 1) for all inputs. Since the gradient of Sigmoid is zero for outputs 0 or 1, the model would not learn. \n",
    "\n",
    "To mitigate this issue I employed two techniques:\n",
    "* First, in the Custom layers we implemented proper weight and initialization $W \\sim \\mathcal{U}\\left(-\\frac{1}{\\sqrt{c_{out}}}, \\frac{1}{\\sqrt{c_{out}}}\\right)$ which is a common practice in deep learning. That partially solved the problem, but not entirely.\n",
    "\n",
    "* Second, I added a `nn.BatchNorm1d(num_features=1)` layer after the `LinearInvariant` layer, right before the sigmoid function. It learns the mean and variance of the outputS of the `LinearInvariant` layer during the training process, and normalizes it to variance 1 and expectancy 0 (on average). It's important to note that the batch normalization layer keeps the model invariant, since it simply multiplies scalar outputs by a learned scalar factor and adds a learned scalar bias. Indeed, using the batch normalization layer solved the problem of the model not learning. Since the batch normalization layer normalizes the inputs to the sigmoid function, it no longer saturates and the model can learn.\n",
    "Although it might take a while for the `BatchNorm1d` layer to learn the proper normalization coefficients, once it does the model trains very quickly. \n",
    "\n",
    "Resulting Architecture:\n",
    "\n",
    "```python\n",
    " def create_invariant_model(n: int, d: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        LinearEquivariant(in_channels=d, out_channels=10),\n",
    "        nn.ReLU(),\n",
    "        LinearEquivariant(in_channels=10, out_channels=10),\n",
    "        nn.ReLU(),\n",
    "        LinearInvariant(in_channels=10, out_channels=1),\n",
    "        nn.BatchNorm1d(1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "```\n",
    "\n",
    "##### Overfitting:\n",
    "\n",
    "Another big issue we encountered was overfitting. To overcome it, we added an option to dynamically generate the data every time the `Dataset` is accessed. \n",
    "This way, the model never sees the same data twice, and not able to overfit. That indeed resolved completely the overfitting issue.\n",
    "\n",
    "##### Symmetrization Network:\n",
    "\n",
    "The symmetrization network is a very powerful tool to learn equivariant functions. However, it is computationally expensive\n",
    "and tricky to implement efficiently. Our implementation balances performance and memory utilization by forwarding through the network multiple \n",
    "permuted versions of the input data at once (by creating a super-batch). We can control that number of forwarded permutations to balance performance and memory (more permutations - better performance but higher memory utilization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 and 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason that the equivariant architecture is especially suited for this task is the following:\n",
    "\n",
    "Given $d$ dimensional random vector $x = (x_1, x_2, ..., x_d) \\sim \\mathcal{D^d}$, the formula for the empirical variance is as follows:\n",
    "\n",
    "$$ {Var}_{em}(x) = \\frac{1}{d - 1} \\sum_{i=1}^{d} (x_i - \\bar{x})^2 $$\n",
    "\n",
    "$$ \\bar{x} = \\frac{1}{d} \\sum_{i=1}^{d} x_i $$\n",
    "\n",
    "Notice that this calculation can be easily achieved by the the following invariant architecture:\n",
    "\n",
    "($I_{d \\times d}$ denotes the identity matrix, $1_{d \\times d}$ denotes the matrix filled with ones, and $0_{d}$ denotes the zero vector)\n",
    "\n",
    "Let $N = \\phi \\circ \\alpha \\circ F$ be an invariant network, where $\\phi$ is invariant layer, $\\alpha$ is pointwise activation, and $F$ is an equivariant layer.\n",
    "\n",
    "* Define $F : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ as follows: $ F = 1 \\cdot I_{d \\times d} + \\frac{-1}{d} \\cdot {1}_{d \\times d} + 0_{d}$\n",
    "(recall that general equivariant $F$ is of the form $ F = \\alpha \\cdot I_{d \\times d} + \\beta \\cdot {1}_{d \\times d} + b$)\n",
    "\n",
    "* Define $\\alpha : \\mathbb{R} \\rightarrow \\mathbb{R}$ as follows: $ \\alpha(x) = x^2 $\n",
    "\n",
    "* Define $\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ as follows: $ \\phi = \\frac{1}{d-1} \\cdot 1_{d} + 0$ (recall that general invariant $\\phi$ is of the form $ \\phi = \\alpha \\cdot 1_{d} + b$)\n",
    "\n",
    "As result, we get:\n",
    "\n",
    "$$ F(x)_i = x_i - \\frac{1}{d} \\sum_{j=1}^{d} x_j + 0 = x_i - \\bar{x} $$\n",
    "\n",
    "$$ \\alpha(F(x))_i = F(x)_i^2 = (x_i - \\bar{x})^2 $$\n",
    "\n",
    "$$ N(x) = \\frac{1}{d-1} \\sum_{i=1}^{d} \\alpha(F(x))_i = \\frac{1}{d-1} \\sum_{i=1}^{d} (x_i - \\bar{x})^2 = \\text{Var}_{em}(x) $$\n",
    "\n",
    "We showed that using the invariant and equivariant layers only, we were able to calculate the empirical variance of an input sample. \n",
    "Since the task of the model is to differentiate between inputs generated from distributions with different variances, the equivariant architecture is especially suited for this task.\n",
    "Notice the small number of parameters required to calculate the variance (3 parameters in layer $F$ and two parameters in layer $\\phi$), hence the optimization process of this architecture is easier than for other architectures.\n",
    "\n",
    "*Note: We showed how to calculate the variance across a single feature dimension, but the same idea holds for calculating the variance for each feature for element of size $(n \\times d)$, and afterwards averaging the empirical variances across the feature dimension to get the final variance of the sample.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we're using the symmetry group $S_n$ over the channel dimensions.\n",
    "A better symmetry group to use would be $S_n \\times S_d$ when $S_n$ acts on the channel dimension and $S_d$ acts on the feature dimension. The reason this symmetry group is suitable is because each feature is a vector of length $d$ generated from a normal distribution, and any permutation of the vector does not change the probability of it being generated, nor the underlying distribution that generated it. Since the model tries to detect the underlying distribution, it should be invariant to permutations of the feature dimensions.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$ \\Pr(x_1, x_2, ... x_n \\sim \\mathcal{N}(0, I) \\; | \\; x_1, x_2, ... x_n) = \n",
    "\\Pr(\\sigma \\cdot x_1, \\sigma \\cdot x_2, ... \\sigma \\cdot x_n \\sim \\mathcal{N}(0, I) \\; | \\; x_1, x_2, ... ,x_n, \\forall \\sigma \\in S_d) $$\n",
    "\n",
    "when $x_i$ is a feature vector of length $d$ and $\\sigma$ is a permutation of the feature dimensions\n",
    "(remember that each input sample is composed of $n$ feature vectors of length $d$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-groups",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

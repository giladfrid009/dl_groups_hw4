{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gaussian_dataset import GaussianDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "N = 10\n",
    "D = 5\n",
    "data_size = 1000\n",
    "\n",
    "train_size = int(0.8 * data_size)\n",
    "test_size = data_size - train_size\n",
    "\n",
    "ds_train = GaussianDataset(\n",
    "    num_samples=train_size,\n",
    "    shape=(N, D),\n",
    "    var1=1.0,\n",
    "    var2=0.8,\n",
    "    static=False,\n",
    ")\n",
    "\n",
    "ds_test = GaussianDataset(\n",
    "    num_samples=test_size,\n",
    "    shape=(N, D),\n",
    "    var1=1.0,\n",
    "    var2=0.8,\n",
    "    static=True,\n",
    ")\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    dataset=ds_train,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "dl_test = DataLoader(\n",
    "    dataset=ds_test,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashlat/miniforge3/envs/dl-groups/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.training import BinaryTrainer\n",
    "from src.permutation import Permutation, create_all_permutations, create_permutations_from_generators\n",
    "from src.layers import LinearEquivariant, LinearInvariant\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonization Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CanonicalModel(nn.Module):\n",
    "    def __init__(self, model: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model.to(device)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = torch.sort(x, dim=-1, descending=True).values\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(in_features=N * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "model = CanonicalModel(layers)\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    "    log=False,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=300,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashlat/miniforge3/envs/dl-groups/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EPOCH 1/300 ---\n",
      "train_batch:   0%|          | 0/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([32])) that is different to the input size (torch.Size([1600])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m CanonicalModel(layers)\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m BinaryTrainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/source/dl_groups/src/training.py:104\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, dl_train, dl_test, num_epochs, checkpoints, early_stopping, print_every, **kw)\u001b[0m\n\u001b[1;32m    100\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- EPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose)\n\u001b[0;32m--> 104\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m test_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_epoch(dl_test, verbose\u001b[38;5;241m=\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    107\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mextend(train_result\u001b[38;5;241m.\u001b[39mlosses)\n",
      "File \u001b[0;32m~/source/dl_groups/src/training.py:157\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self, dl_train, **kw)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03mTrain once over a training set (single epoch).\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    EpochResult: An EpochResult for the epoch.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# set train mode\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/source/dl_groups/src/training.py:245\u001b[0m, in \u001b[0;36mTrainer._foreach_batch\u001b[0;34m(dl, forward_fn, verbose, max_batches)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m    244\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dl_iter)\n\u001b[0;32m--> 245\u001b[0m     batch_res \u001b[38;5;241m=\u001b[39m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpbar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_res\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    248\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/source/dl_groups/src/training.py:312\u001b[0m, in \u001b[0;36mBinaryTrainer.train_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    310\u001b[0m preds: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[1;32m    311\u001b[0m preds \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m--> 312\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniforge3/envs/dl-groups/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/dl-groups/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/dl-groups/lib/python3.11/site-packages/torch/nn/modules/loss.py:621\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/dl-groups/lib/python3.11/site-packages/torch/nn/functional.py:3163\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3161\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3164\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3166\u001b[0m     )\n\u001b[1;32m   3168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3169\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([1600])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "from src.layers import PositionalEncoding\n",
    "\n",
    "layers = nn.Sequential(\n",
    "    PositionalEncoding(d_model=D, max_len=N),\n",
    "    nn.TransformerEncoder(\n",
    "        nn.TransformerEncoderLayer(batch_first=True, d_model=D, nhead=1),\n",
    "        num_layers=2,\n",
    "    ),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "model = CanonicalModel(layers)\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    "    log=False,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=300,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetrization Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Callable, Iterator\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class SymmetryModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        perm_creator: Callable[[None], Iterator[Permutation]],\n",
    "        chunksize: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.perm_creator = perm_creator\n",
    "        self.chunksize = chunksize\n",
    "\n",
    "    def _chunk(self, data: Iterable[Permutation], chunksize: int) -> Iterable[list[Permutation]]:\n",
    "        data_iter: Iterable[Permutation] = iter(data)\n",
    "        buffer: deque[Permutation] = deque()\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                buffer.append(next(data_iter))\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            if len(buffer) == chunksize:\n",
    "                yield list(buffer)\n",
    "                buffer.clear()\n",
    "\n",
    "        if buffer:\n",
    "            yield list(buffer)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        total = 0\n",
    "        result = None\n",
    "\n",
    "        perms = self.perm_creator()\n",
    "        for perm_chunk in self._chunk(perms, self.chunksize):\n",
    "            chunksize = len(perm_chunk)\n",
    "            total += chunksize\n",
    "            permuted = torch.vstack([perm(x) for perm in perm_chunk])\n",
    "\n",
    "            output: Tensor = self.model.forward(permuted)\n",
    "            output = output.reshape(chunksize, output.shape[0] // chunksize, *output.shape[1:])\n",
    "            output = torch.sum(output, dim=0)\n",
    "\n",
    "            if result is None:\n",
    "                result = output\n",
    "            else:\n",
    "                result = result + output\n",
    "\n",
    "        result = result / total\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(in_features=N * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "shift_perm = torch.arange(N) + 1\n",
    "shift_perm[-1] = 0\n",
    "\n",
    "model = SymmetryModel(\n",
    "    layers,\n",
    "    perm_creator=lambda: create_permutations_from_generators([Permutation(shift_perm)]),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.01),\n",
    "    device=device,\n",
    "    log=False,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=300,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampled Symmetrization Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(in_features=N * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "num = int(math.factorial(N) * 0.05)\n",
    "num = 10\n",
    "\n",
    "model = SymmetryModel(\n",
    "    layers,\n",
    "    perm_creator=lambda: (Permutation(torch.randperm(N, dtype=torch.long)) for _ in range(num)),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    "    log=False,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=300,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Equivariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    LinearEquivariant(in_channels=D, out_channels=10),\n",
    "    nn.ReLU(),\n",
    "    LinearEquivariant(in_channels=10, out_channels=10),\n",
    "    nn.ReLU(),\n",
    "    LinearInvariant(in_channels=10, out_channels=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    "    log=False,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=300,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentation(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Randomly permute the input tensor along the channel dimension.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, d, channel)\n",
    "        \"\"\"\n",
    "        rnd = torch.randn_like(x)\n",
    "        indices = rnd.argsort(dim=-1)\n",
    "        result = torch.gather(x, -1, indices)\n",
    "        return result\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Augmentation(),\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(in_features=N * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    "    log=False,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=300,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Question 4: Challenges encountered during Implementation:\n",
    "\n",
    "##### Numeric Errors:\n",
    "\n",
    "The first challenge encountered is in the implementation of the invariant and equivariant layers.\n",
    "The main implementation challenge rose from the fact that in the lecture, the equivariant layer is formulated as follows:\n",
    "\n",
    "$$ F(x) : \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d'} $$\n",
    "\n",
    "$$ F(x)_j = \\sum _{i=1} ^ {d} L_{ij}(x) $$ \n",
    "where $L_{ij}(x)$ is a single feature linear equivariant layer.\n",
    "\n",
    "Technically, this implementation is indeed correct, but the summation over all $L_{ij}(x)$ might causes layer outputs to blow-up.  \n",
    "As result, the outputs of the $F \\circ a \\circ F ...$ become very large.\n",
    "\n",
    "Our network is composed of these layers $\\phi \\circ F \\circ a \\circ F ...$, when $\\phi$ is the sigmoid function that returns values between 0 and 1.\n",
    "\n",
    "Since the last layer of the network is a sigmoid function, and the results of the previous layers are very large (their absolute value), the sigmoid function saturates and returns either 0.0 or 1.0. Because the sigmoid function got saturated, the propagated gradients become 0, hence the network does not learn.\n",
    "\n",
    "To resolve this issue we defined the equivariant layer as follows:\n",
    "\n",
    "$$ F(x)_j = \\frac{1}{d} \\sum _{i=1} ^ {d} L_{ij}(x) $$ \n",
    "\n",
    "This formulation still retains the equivariance property, but it prevents the layer outputs from blowing-up.\n",
    "\n",
    "*Note: We applied the same averaging technique to the invariant layers as well.*\n",
    "\n",
    "##### Overfitting:\n",
    "\n",
    "Another big issue we encountered was overfitting. To overcome it, we added an option to dynamically generate the data every time the `Dataset` is accessed. \n",
    "This way, the model never sees the same data twice, and not able to overfit. That indeed resolved completely the overfitting issue.\n",
    "For the comparative analysis, we didn't use this option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8:\n",
    "\n",
    "Currently, we're using the symmetry group $S_n$ over the channel dimensions.\n",
    "A better symmetry group to use would be $S_n \\times S_d$ when $S_n$ acts on the channel dimension and $S_d$ acts on the feature dimension. The reason this symmetry group is suitable is because each feature is a vector of length $d$ generated from a normal distribution, and any permutation of the vector does not change the probability of it being generated, nor the underlying distribution that generated it. Since the model tries to detect the underlying distribution, it should be invariant to permutations of the feature dimensions.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$ \\Pr(x_1, x_2, ... x_n \\sim \\mathcal{N}(0, I) \\; | \\; x_1, x_2, ... x_n) = \n",
    "\\Pr(\\sigma \\cdot x_1, \\sigma \\cdot x_2, ... \\sigma \\cdot x_n \\sim \\mathcal{N}(0, I) \\; | \\; x_1, x_2, ... ,x_n, \\forall \\sigma \\in S_d) $$\n",
    "\n",
    "when $x_i$ is a feature vector of length $d$ and $\\sigma$ is a permutation of the feature dimensions\n",
    "(remember that each input sample is composed of $n$ feature vectors of length $d$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-groups",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

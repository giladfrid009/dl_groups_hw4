{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Permutation(nn.Module):\n",
    "    def __init__(self, n: int, perm: torch.Tensor) -> None:\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.perm = perm.clone().detach()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if x.ndim == 1:\n",
    "            return x[self.perm]\n",
    "        return x[:, self.perm]\n",
    "\n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        if isinstance(other, Permutation):\n",
    "            return self.n == other.n and torch.all(self.perm == other.perm)\n",
    "        return False\n",
    "\n",
    "    def __ne__(self, value: object) -> bool:\n",
    "        return not self.__eq__(value)\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return hash((self.n, self.perm.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import deque\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def create_all_permutations(n: int) -> Iterator[Permutation]:\n",
    "    for perm in itertools.permutations(range(n)):\n",
    "        yield Permutation(n, torch.tensor(perm, dtype=torch.long))\n",
    "\n",
    "\n",
    "def create_permutations_from_generators(n: int, generators: list[Permutation]) -> Iterator[Permutation]:\n",
    "    assert all(perm.n == n for perm in generators)\n",
    "\n",
    "    def compose(p1: Permutation, p2: Permutation) -> Permutation:\n",
    "        return Permutation(n, p1.forward(p2.perm))\n",
    "\n",
    "    identity = Permutation(n, torch.arange(n))\n",
    "    generated_perms = {identity}\n",
    "    queue = deque([identity])\n",
    "\n",
    "    yield identity\n",
    "\n",
    "    while queue:\n",
    "        current_perm = queue.popleft()\n",
    "        for gen in generators:\n",
    "            new_perm = compose(current_perm, gen)\n",
    "            if new_perm not in generated_perms:\n",
    "                generated_perms.add(new_perm)\n",
    "                queue.append(new_perm)\n",
    "                yield new_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples: int, d: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.d = d\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        labels = torch.randint(0, 2, (num_samples, 1))\n",
    "        data = torch.randn(num_samples, d)\n",
    "\n",
    "        variance = 0.1 * (labels == 0) + 10.0 * (labels != 0)\n",
    "        data = data * variance\n",
    "\n",
    "        self.data = data.to(torch.float32)\n",
    "        self.labels = labels.to(torch.float32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tensor:\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CanonicalModel(nn.Module):\n",
    "    def __init__(self, d: int, model: nn.Module, device: torch.device) -> None:\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = torch.sort(x, dim=-1, descending=True).values\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetryModel(nn.Module):\n",
    "    def __init__(self, d: int, perms: Iterator[Permutation], model: nn.Module, device: torch.device) -> None:\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.perms = perms\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        permuted_x = torch.stack([perm(x) for perm in self.perms])\n",
    "        outputs = self.model(permuted_x)\n",
    "        return torch.mean(outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEquivariant(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.b = torch.nn.Parameter(torch.randn(1))\n",
    "        self.alpha = torch.nn.Parameter(torch.randn(1))\n",
    "        self.beta = torch.nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        result = self.beta * x + self.alpha * torch.sum(x, dim=-1, keepdim=True) + self.b\n",
    "        return result\n",
    "\n",
    "\n",
    "class LinearInvariant(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.b = torch.nn.Parameter(torch.randn(1))\n",
    "        self.alpha = torch.nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        result = self.alpha * torch.sum(x, dim=-1, keepdim=True) + self.b\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_samples = 1000\n",
    "hidden_dim = 500\n",
    "\n",
    "train_size = int(0.8 * num_samples)\n",
    "test_size = num_samples - train_size\n",
    "\n",
    "ds_train = GaussianDataset(train_size, hidden_dim)\n",
    "ds_test = GaussianDataset(test_size, hidden_dim)\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    dataset=ds_train,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "dl_test = DataLoader(\n",
    "    dataset=ds_test,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import RegularTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: DOES NOT TRAIN WTF :C\n",
    "# NONE OF THE NETWORKS EXHIBIT ANY FORM OF LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    LinearEquivariant(),\n",
    "    nn.ReLU(),\n",
    "    LinearEquivariant(),\n",
    "    nn.ReLU(),\n",
    "    LinearInvariant(),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "intrinsic_trainer = RegularTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.SGD(model.parameters(), lr=0.01),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "intrinsic_trainer.fit(dl_train=dl_train, dl_test=dl_test, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Sequential(\n",
    "    nn.Linear(hidden_dim, hidden_dim, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, hidden_dim, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, 1, bias=True),\n",
    "    nn.Sigmoid(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CanonicalModel(hidden_dim, layers, device)\n",
    "\n",
    "canonical_trainer = RegularTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.01),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "canonical_trainer.fit(dl_train=dl_train, dl_test=dl_test, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(hidden_dim, hidden_dim, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, hidden_dim, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, 1, bias=True),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "perms = list(create_all_permutations(hidden_dim))\n",
    "\n",
    "model = SymmetryModel(hidden_dim, perms, layers, device)\n",
    "\n",
    "symmetry_trainer = RegularTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.SGD(model.parameters(), lr=0.01),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "symmetry_trainer.fit(dl_train=dl_train, dl_test=dl_test, num_epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-groups",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

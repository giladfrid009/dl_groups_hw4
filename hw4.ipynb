{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gaussian_dataset import GaussianDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "N = 10\n",
    "D = 5\n",
    "data_samples = 1000\n",
    "\n",
    "train_size = int(0.8 * data_samples)\n",
    "test_size = data_samples - train_size\n",
    "\n",
    "ds_train = GaussianDataset(num_samples=train_size, shape=(N, D), var1=1.0, var2=0.8, static=False)\n",
    "\n",
    "ds_test = GaussianDataset(num_samples=test_size, shape=(N, D), var1=1.0, var2=0.8, static=True)\n",
    "\n",
    "dl_train = DataLoader(dataset=ds_train, batch_size=32, shuffle=False)\n",
    "\n",
    "dl_test = DataLoader(dataset=ds_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import test_invariant, test_equivariant\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training import BinaryTrainer\n",
    "from src.layers import LinearEquivariant, LinearInvariant, PositionalEncoding\n",
    "\n",
    "\n",
    "def create_mlp_model(n: int, d: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(start_dim=1),\n",
    "        nn.Linear(in_features=n * d, out_features=10 * d),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=10 * d, out_features=10 * d),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=10 * d, out_features=1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_transformer_model(n: int, d: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        PositionalEncoding(d_model=d, max_len=n),\n",
    "        nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(batch_first=True, d_model=d, nhead=1),\n",
    "            norm=nn.LayerNorm(normalized_shape=d),\n",
    "            num_layers=1,\n",
    "        ),\n",
    "        nn.Flatten(start_dim=1),\n",
    "        nn.Linear(in_features=n * d, out_features=1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_invariant_model(n: int, d: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        LinearEquivariant(in_channels=d, out_channels=10),\n",
    "        nn.ReLU(),\n",
    "        LinearEquivariant(in_channels=10, out_channels=10),\n",
    "        nn.ReLU(),\n",
    "        LinearInvariant(in_channels=10, out_channels=1),\n",
    "        nn.Sigmoid(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, epochs: int = 200, log: bool = True):\n",
    "    trainer = BinaryTrainer(\n",
    "        model=model,\n",
    "        loss_fn=nn.BCELoss(),\n",
    "        optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "        device=device,\n",
    "        log=log,\n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        dl_train=dl_train,\n",
    "        dl_test=dl_test,\n",
    "        num_epochs=epochs,\n",
    "        print_every=25,\n",
    "        early_stopping=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonization Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import CanonicalModel\n",
    "\n",
    "model = CanonicalModel(create_mlp_model(N, D))\n",
    "\n",
    "test_invariant(model, input=torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CanonicalModel(create_transformer_model(N, D))\n",
    "\n",
    "test_invariant(model, input=torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetrization Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.permutation import Permutation, create_all_permutations, create_permutations_from_generators\n",
    "from src.models import SymmetryModel\n",
    "\n",
    "shift_perm = Permutation((torch.arange(N) + 1) % N)\n",
    "\n",
    "model = SymmetryModel(\n",
    "    model=create_mlp_model(N, D),\n",
    "    perm_creator=lambda: create_permutations_from_generators([shift_perm]),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_perm = Permutation((torch.arange(N) + 1) % N)\n",
    "\n",
    "model = SymmetryModel(\n",
    "    model=create_transformer_model(N, D),\n",
    "    perm_creator=lambda: create_permutations_from_generators([shift_perm]),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP-Based Sampled Symmetrization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = int(math.factorial(N) * 0.05)\n",
    "num = 10\n",
    "\n",
    "model = SymmetryModel(\n",
    "    model=create_mlp_model(N, D),\n",
    "    perm_creator=lambda: (Permutation(torch.randperm(N)) for _ in range(num)),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intrinsic Invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_invariant_model(N, D)\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentation(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Randomly permute the input tensor along the channel dimension.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, d, channel)\n",
    "        \"\"\"\n",
    "        rnd = torch.randn_like(x)\n",
    "        indices = rnd.argsort(dim=-1)\n",
    "        result = torch.gather(x, -1, indices)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Augmentation(),\n",
    "    create_invariant_model(N, D),\n",
    ")\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Question 4: Challenges encountered during Implementation:\n",
    "\n",
    "##### Numeric Errors:\n",
    "\n",
    "The first challenge encountered is in the implementation of the invariant and equivariant layers.\n",
    "The main implementation challenge rose from the fact that in the lecture, the equivariant layer is formulated as follows:\n",
    "\n",
    "$$ F(x) : \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d'} $$\n",
    "\n",
    "$$ F(x)_j = \\sum _{i=1} ^ {d} L_{ij}(x) $$ \n",
    "where $L_{ij}(x)$ is a single feature linear equivariant layer.\n",
    "\n",
    "Technically, this implementation is indeed correct, but the summation over all $L_{ij}(x)$ might causes layer outputs to blow-up.  \n",
    "As result, the outputs of the $F \\circ a \\circ F ...$ become very large.\n",
    "\n",
    "Our network is composed of these layers $\\phi \\circ F \\circ a \\circ F ...$, when $\\phi$ is the sigmoid function that returns values between 0 and 1.\n",
    "\n",
    "Since the last layer of the network is a sigmoid function, and the results of the previous layers are very large (their absolute value), the sigmoid function saturates and returns either 0.0 or 1.0. Because the sigmoid function got saturated, the propagated gradients become 0, hence the network does not learn.\n",
    "\n",
    "To resolve this issue we defined the equivariant layer as follows:\n",
    "\n",
    "$$ F(x)_j = \\frac{1}{d} \\sum _{i=1} ^ {d} L_{ij}(x) $$ \n",
    "\n",
    "This formulation still retains the equivariance property, but it prevents the layer outputs from blowing-up.\n",
    "\n",
    "*Note: We applied the same averaging technique to the invariant layers as well.*\n",
    "\n",
    "##### Overfitting:\n",
    "\n",
    "Another big issue we encountered was overfitting. To overcome it, we added an option to dynamically generate the data every time the `Dataset` is accessed. \n",
    "This way, the model never sees the same data twice, and not able to overfit. That indeed resolved completely the overfitting issue.\n",
    "For the comparative analysis, we didn't use this option.\n",
    "\n",
    "##### Symmetrization Network:\n",
    "\n",
    "The symmetrization network is a very powerful tool to learn equivariant functions. However, it is computationally expensive\n",
    "and tricky to implement efficiently. Our implementation balances performance and memory utilization by forwarding through the network multiple \n",
    "permuted versions of the input data at once (by creating a super-batch). We can control that number of forwarded permutations to balance performance and memory (more permutations - better performance but higher memory utilization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8:\n",
    "\n",
    "Currently, we're using the symmetry group $S_n$ over the channel dimensions.\n",
    "A better symmetry group to use would be $S_n \\times S_d$ when $S_n$ acts on the channel dimension and $S_d$ acts on the feature dimension. The reason this symmetry group is suitable is because each feature is a vector of length $d$ generated from a normal distribution, and any permutation of the vector does not change the probability of it being generated, nor the underlying distribution that generated it. Since the model tries to detect the underlying distribution, it should be invariant to permutations of the feature dimensions.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$ \\Pr(x_1, x_2, ... x_n \\sim \\mathcal{N}(0, I) \\; | \\; x_1, x_2, ... x_n) = \n",
    "\\Pr(\\sigma \\cdot x_1, \\sigma \\cdot x_2, ... \\sigma \\cdot x_n \\sim \\mathcal{N}(0, I) \\; | \\; x_1, x_2, ... ,x_n, \\forall \\sigma \\in S_d) $$\n",
    "\n",
    "when $x_i$ is a feature vector of length $d$ and $\\sigma$ is a permutation of the feature dimensions\n",
    "(remember that each input sample is composed of $n$ feature vectors of length $d$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-groups",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

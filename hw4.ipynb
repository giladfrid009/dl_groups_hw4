{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Permutation(nn.Module):\n",
    "    def __init__(self, perm: torch.Tensor) -> None:\n",
    "        super().__init__()\n",
    "        self.perm = perm.clone().detach()\n",
    "        self.hash = hash(tuple(perm.tolist()))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if x.ndim == 1:\n",
    "            return x[self.perm]\n",
    "        return x[:, self.perm]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.perm)\n",
    "\n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        if isinstance(other, Permutation):\n",
    "            return torch.all(self.perm == other.perm)\n",
    "        return False\n",
    "\n",
    "    def __ne__(self, value: object) -> bool:\n",
    "        return not self.__eq__(value)\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return self.hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import deque\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def create_all_permutations(perm_length: int) -> Iterator[Permutation]:\n",
    "    for perm in itertools.permutations(range(perm_length)):\n",
    "        yield Permutation(torch.tensor(perm, dtype=torch.long))\n",
    "\n",
    "\n",
    "def create_permutations_from_generators(generators: list[Permutation]) -> Iterator[Permutation]:\n",
    "    def compose(p1: Permutation, p2: Permutation) -> Permutation:\n",
    "        return Permutation(p1.forward(p2.perm))\n",
    "\n",
    "    length = len(generators[0])\n",
    "    id = Permutation(torch.arange(length))\n",
    "    generated_perms = {id}\n",
    "    queue = deque([id])\n",
    "\n",
    "    yield id\n",
    "\n",
    "    while queue:\n",
    "        current_perm = queue.popleft()\n",
    "        for gen in generators:\n",
    "            new_perm = compose(current_perm, gen)\n",
    "            if new_perm not in generated_perms:\n",
    "                generated_perms.add(new_perm)\n",
    "                queue.append(new_perm)\n",
    "                yield new_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CanonicalModel(nn.Module):\n",
    "    def __init__(self, model: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model.to(device)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = torch.sort(x, dim=-1, descending=True).values\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetryModel(nn.Module):\n",
    "    def __init__(self, model: nn.Module, perms: Iterator[Permutation]) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.perms = perms\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        permuted_x = torch.stack([perm(x) for perm in self.perms])\n",
    "        outputs = self.model(permuted_x)\n",
    "        return torch.mean(outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ADD in_features and out_features params\n",
    "class LinearEquivariant(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.b = torch.nn.Parameter(torch.randn(1))\n",
    "        self.alpha = torch.nn.Parameter(torch.randn(1))\n",
    "        self.beta = torch.nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        result = self.beta * x + self.alpha * torch.sum(x, dim=-1, keepdim=True) + self.b\n",
    "        return result\n",
    "\n",
    "\n",
    "# TODO ADD in_features and out_features params\n",
    "class LinearInvariant(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.b = torch.nn.Parameter(torch.randn(1))\n",
    "        self.alpha = torch.nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        result = self.alpha * torch.sum(x, dim=-1, keepdim=True) + self.b\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: GENERATE SYNTHETIC DATA IN REAL TIME TO AVOID OVERFITTING ON TRAIN\n",
    "class GaussianDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, length: int, dim: int, var1: float = 1.0, var2: float = 0.8) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        labels = torch.randint(0, 2, (length, 1))\n",
    "        data = torch.randn(length, dim)\n",
    "\n",
    "        variance = math.sqrt(var1) * (labels == 0) + math.sqrt(var2) * (labels != 0)\n",
    "        data = data * variance\n",
    "\n",
    "        self.data = data.to(torch.float32)\n",
    "        self.labels = labels.to(torch.float32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[Tensor, Tensor]:\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_size = 1000\n",
    "hidden_dim = 5\n",
    "\n",
    "train_size = int(0.8 * data_size)\n",
    "test_size = data_size - train_size\n",
    "\n",
    "ds_train = GaussianDataset(train_size, hidden_dim, var1=1.0, var2=0.8)\n",
    "ds_test = GaussianDataset(test_size, hidden_dim, var1=1.0, var2=0.8)\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    dataset=ds_train,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "dl_test = DataLoader(\n",
    "    dataset=ds_test,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import BinaryTrainer\n",
    "\n",
    "# TODO: DOES NOT TRAIN WTF :C\n",
    "# NONE OF THE NETWORKS EXHIBIT ANY FORM OF LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    LinearEquivariant(),\n",
    "    nn.ReLU(),\n",
    "    LinearEquivariant(),\n",
    "    nn.ReLU(),\n",
    "    LinearInvariant(),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=100,\n",
    "    print_every=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Sequential(\n",
    "    nn.Linear(hidden_dim, 64, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1, bias=True),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "model = CanonicalModel(layers)\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    "    log=True,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=1000,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Sequential(\n",
    "    nn.Linear(hidden_dim, hidden_dim, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, hidden_dim, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, 1, bias=True),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "perms = list(create_permutations_from_generators([Permutation(torch.arange(hidden_dim))]))\n",
    "\n",
    "model = SymmetryModel(layers, perms)\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "trainer.fit(dl_train=dl_train, dl_test=dl_test, num_epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-groups",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

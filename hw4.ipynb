{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gaussian_dataset import GaussianDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "N = 10\n",
    "D = 5\n",
    "data_size = 1000\n",
    "\n",
    "train_size = int(0.8 * data_size)\n",
    "test_size = data_size - train_size\n",
    "\n",
    "ds_train = GaussianDataset(\n",
    "    num_samples=train_size,\n",
    "    shape=(N, D),\n",
    "    var1=1.0,\n",
    "    var2=0.8,\n",
    "    static=False,\n",
    ")\n",
    "\n",
    "ds_test = GaussianDataset(\n",
    "    num_samples=test_size,\n",
    "    shape=(N, D),\n",
    "    var1=1.0,\n",
    "    var2=0.8,\n",
    "    static=True,\n",
    ")\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    dataset=ds_train,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "dl_test = DataLoader(\n",
    "    dataset=ds_test,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training import BinaryTrainer\n",
    "from src.permutation import Permutation, create_all_permutations, create_permutations_from_generators\n",
    "from src.layers import LinearEquivariant, LinearInvariant\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonization Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CanonicalModel(nn.Module):\n",
    "    def __init__(self, model: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model.to(device)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = torch.sort(x, dim=-1, descending=True).values\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(in_features=N * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "model = CanonicalModel(layers)\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    "    log=False,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=300,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetrization Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Callable, Iterator\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class SymmetryModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        perm_creator: Callable[[None], Iterator[Permutation]],\n",
    "        chunksize: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.perm_creator = perm_creator\n",
    "        self.chunksize = chunksize\n",
    "\n",
    "    def _chunk(self, data: Iterable[Permutation], chunksize: int) -> Iterable[list[Permutation]]:\n",
    "        data_iter: Iterable[Permutation] = iter(data)\n",
    "        buffer: deque[Permutation] = deque()\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                buffer.append(next(data_iter))\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            if len(buffer) == chunksize:\n",
    "                yield list(buffer)\n",
    "                buffer.clear()\n",
    "\n",
    "        if buffer:\n",
    "            yield list(buffer)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        total = 0\n",
    "        result = None\n",
    "\n",
    "        perms = self.perm_creator()\n",
    "        for perm_chunk in self._chunk(perms, self.chunksize):\n",
    "            chunksize = len(perm_chunk)\n",
    "            total += chunksize\n",
    "            permuted = torch.vstack([perm(x) for perm in perm_chunk])\n",
    "\n",
    "            output: Tensor = self.model.forward(permuted)\n",
    "            output = output.reshape(chunksize, output.shape[0] // chunksize, *output.shape[1:])\n",
    "            output = torch.sum(output, dim=0)\n",
    "\n",
    "            if result is None:\n",
    "                result = output\n",
    "            else:\n",
    "                result = result + output\n",
    "\n",
    "        result = result / total\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(in_features=N * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "shift_perm = torch.arange(N) + 1\n",
    "shift_perm[-1] = 0\n",
    "\n",
    "model = SymmetryModel(\n",
    "    layers,\n",
    "    perm_creator=lambda: create_permutations_from_generators([Permutation(shift_perm)]),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.01),\n",
    "    device=device,\n",
    "    log=False,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=300,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampled Symmetrization Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(in_features=N * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "num = int(math.factorial(N) * 0.05)\n",
    "num = 10\n",
    "\n",
    "model = SymmetryModel(\n",
    "    layers,\n",
    "    perm_creator=lambda: (Permutation(torch.randperm(N, dtype=torch.long)) for _ in range(num)),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    "    log=False,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=300,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Equivariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    LinearEquivariant(in_channels=D, out_channels=10),\n",
    "    nn.ReLU(),\n",
    "    LinearEquivariant(in_channels=10, out_channels=10),\n",
    "    nn.ReLU(),\n",
    "    LinearInvariant(in_channels=10, out_channels=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    "    log=False,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=300,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentation(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Randomly permute the input tensor along the channel dimension.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, d, channel)\n",
    "        \"\"\"\n",
    "        rnd = torch.randn_like(x)\n",
    "        indices = rnd.argsort(dim=-1)\n",
    "        result = torch.gather(x, -1, indices)\n",
    "        return result\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Augmentation(),\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(in_features=N * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=10 * D),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10 * D, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    "    log=False,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=300,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Challenges encountered during Implementation:\n",
    "\n",
    "##### Numeric Errors:\n",
    "\n",
    "The first challenge encountered is in the implementation of the invariant and equivariant layers.\n",
    "The main implementation challenge rose from the fact that in the lecture, the equivariant layer is formulated as follows:\n",
    "\n",
    "$$ F(x) : \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d'} $$\n",
    "\n",
    "$$ F(x)_j = \\sum _{i=1} ^ {d} L_{ij}(x) $$ \n",
    "where $L_{ij}(x)$ is a single feature linear equivariant layer.\n",
    "\n",
    "Technically, this implementation is indeed correct, but the summation over all $L_{ij}(x)$ might causes layer outputs to blow-up.  \n",
    "As result, the outputs of the $F \\circ a \\circ F ...$ become very large.\n",
    "\n",
    "Our network is composed of these layers $\\phi \\circ F \\circ a \\circ F ...$, when $\\phi$ is the sigmoid function that returns values between 0 and 1.\n",
    "\n",
    "Since the last layer of the network is a sigmoid function, and the results of the previous layers are very large (their absolute value), the sigmoid function saturates and returns either 0.0 or 1.0. Because the sigmoid function got saturated, the propagated gradients become 0, hence the network does not learn.\n",
    "\n",
    "To resolve this issue we defined the equivariant layer as follows:\n",
    "\n",
    "$$ F(x)_j = \\frac{1}{d} \\sum _{i=1} ^ {d} L_{ij}(x) $$ \n",
    "\n",
    "This formulation still retains the equivariance property, but it prevents the layer outputs from blowing-up.\n",
    "\n",
    "*Note: We applied the same averaging technique to the invariant layers as well.*\n",
    "\n",
    "##### Overfitting:\n",
    "\n",
    "Another big issue we encountered was overfitting. To overcome it, we added an option to dynamically generate the data every time the `Dataset` is accessed. \n",
    "This way, the model never sees the same data twice, and not able to overfit. That indeed resolved completely the overfitting issue.\n",
    "For the comparative analysis, we didn't use this option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8:\n",
    "\n",
    "Currently, we're using the symmetry group $S_n$ over the channel dimensions.\n",
    "A better symmetry group to use would be $S_n \\times S_d$ when $S_n$ acts on the channel dimension and $S_d$ acts on the feature dimension. The reason this symmetry group is suitable is because each feature is a vector of length $d$ generated from a normal distribution, and any permutation of the vector does not change the probability of it being generated, nor the underlying distribution that generated it. Since the model tries to detect the underlying distribution, it should be invariant to permutations of the feature dimensions.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$ \\Pr(x_1, x_2, ... x_n \\sim \\mathcal{N}(0, I) \\; | \\; x_1, x_2, ... x_n) = \n",
    "\\Pr(\\sigma \\cdot x_1, \\sigma \\cdot x_2, ... \\sigma \\cdot x_n \\sim \\mathcal{N}(0, I) \\; | \\; x_1, x_2, ... ,x_n, \\forall \\sigma \\in S_d) $$\n",
    "\n",
    "when $x_i$ is a feature vector of length $d$ and $\\sigma$ is a permutation of the feature dimensions\n",
    "(remember that each input sample is composed of $n$ feature vectors of length $d$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-groups",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

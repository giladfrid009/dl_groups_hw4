{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Permutation(nn.Module):\n",
    "    def __init__(self, perm: torch.Tensor) -> None:\n",
    "        super().__init__()\n",
    "        self.perm = perm.clone().detach()\n",
    "        self.hash = hash(tuple(perm.tolist()))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if x.ndim == 1:\n",
    "            return x[self.perm]\n",
    "        return x[:, self.perm]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.perm)\n",
    "\n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        if isinstance(other, Permutation):\n",
    "            return torch.all(self.perm == other.perm)\n",
    "        return False\n",
    "\n",
    "    def __ne__(self, value: object) -> bool:\n",
    "        return not self.__eq__(value)\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return self.hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import deque\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def create_all_permutations(perm_length: int) -> Iterator[Permutation]:\n",
    "    for perm in itertools.permutations(range(perm_length)):\n",
    "        yield Permutation(torch.tensor(perm, dtype=torch.long))\n",
    "\n",
    "\n",
    "def create_permutations_from_generators(generators: list[Permutation]) -> Iterator[Permutation]:\n",
    "    def compose(p1: Permutation, p2: Permutation) -> Permutation:\n",
    "        return Permutation(p1.forward(p2.perm))\n",
    "\n",
    "    length = len(generators[0])\n",
    "    id = Permutation(torch.arange(length))\n",
    "    generated_perms = {id}\n",
    "    queue = deque([id])\n",
    "\n",
    "    yield id\n",
    "\n",
    "    while queue:\n",
    "        current_perm = queue.popleft()\n",
    "        for gen in generators:\n",
    "            new_perm = compose(current_perm, gen)\n",
    "            if new_perm not in generated_perms:\n",
    "                generated_perms.add(new_perm)\n",
    "                queue.append(new_perm)\n",
    "                yield new_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CanonicalModel(nn.Module):\n",
    "    def __init__(self, model: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model.to(device)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = torch.sort(x, dim=-1, descending=True).values\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetryModel(nn.Module):\n",
    "    def __init__(self, model: nn.Module, perms: Iterator[Permutation]) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.perms = perms\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        permuted_x = torch.stack([perm(x) for perm in self.perms])\n",
    "        outputs = self.model(permuted_x)\n",
    "        return torch.mean(outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEquivariant(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a LinearEquivariant module.\n",
    "        This module is a custom linear layer that is equivariant to permutations of the input.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.b = torch.nn.Parameter(torch.randn(in_channels, out_channels))\n",
    "        self.alpha = torch.nn.Parameter(torch.randn(in_channels, out_channels))\n",
    "        self.beta = torch.nn.Parameter(torch.randn(in_channels, out_channels))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the LinearEquivariant module.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, d, in_channels).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, d, out_channels).\n",
    "        \"\"\"\n",
    "\n",
    "        assert x.ndim == 3\n",
    "        assert x.shape[-1] == self.in_channels\n",
    "\n",
    "        # shape (batch_size, d, in_channels, 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "\n",
    "        # shape (batch_size, 1, in_channels, 1)\n",
    "        x_sum = torch.sum(x, dim=1, keepdim=True)\n",
    "\n",
    "        # shape (batch_size, d, in_channels, out_channels)\n",
    "        all = x * self.alpha + x_sum * self.beta + self.b\n",
    "\n",
    "        # shape (batch_size, d, out_channels)\n",
    "        reduced = torch.mean(all, dim=2)\n",
    "\n",
    "        return reduced\n",
    "\n",
    "    def _forward_manual(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Performs the forward pass manually using hard-coded loops.\n",
    "        * FOR TESTING PURPOSES ONLY. DO NOT USE IN PRODUCTION.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, hdim, in_channels).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, hdim, out_channels).\n",
    "        \"\"\"\n",
    "\n",
    "        assert x.ndim == 3\n",
    "        assert x.shape[-1] == self.in_channels\n",
    "\n",
    "        batch_size, hdim, in_channels = x.shape\n",
    "        out_channels = self.out_channels\n",
    "\n",
    "        # shape (batch_size, in_channels)\n",
    "        x_sum = torch.sum(x, dim=1)\n",
    "\n",
    "        # shape (batch_size, hdim, out_channels)\n",
    "        result = torch.zeros(x.shape[0], x.shape[1], self.out_channels)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c_out in range(out_channels):\n",
    "                for c_in in range(in_channels):\n",
    "                    alpha = self.alpha[c_in, c_out]\n",
    "                    beta = self.beta[c_in, c_out]\n",
    "                    bias = self.b[c_in, c_out]\n",
    "                    res = x[b, :, c_in] * alpha + x_sum[b, c_in] * beta + bias\n",
    "                    result[b, :, c_out] += res\n",
    "\n",
    "        result = result / in_channels\n",
    "        return result\n",
    "\n",
    "\n",
    "class LinearInvariant(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the LinearInvariant module.\n",
    "        This module is a custom linear layer that is invariant to permutations of the input.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.b = torch.nn.Parameter(torch.randn(in_channels, out_channels))\n",
    "        self.alpha = torch.nn.Parameter(torch.randn(in_channels, out_channels))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the LinearInvariant module.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, d, in_channels).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, 1, out_channels).\n",
    "        \"\"\"\n",
    "        assert x.ndim == 3\n",
    "        assert x.shape[-1] == self.in_channels\n",
    "\n",
    "        # shape (batch_size, d, in_channels, 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "\n",
    "        # shape (batch_size, 1, in_channels, 1)\n",
    "        x_sum = torch.sum(x, dim=1, keepdim=True)\n",
    "\n",
    "        # shape (batch_size, 1, in_channels, out_channels)\n",
    "        all = x_sum * self.alpha + self.b\n",
    "\n",
    "        # shape (batch_size, 1, out_channels)\n",
    "        reduced = torch.mean(all, dim=2)\n",
    "\n",
    "        return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearMultiChannel(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, in_features: int, out_features: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            in_features (int): Number of input features.\n",
    "            out_features (int): Number of output features.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.b = torch.nn.Parameter(torch.randn(in_channels, out_channels, out_features, in_features))\n",
    "        self.weights = torch.nn.Parameter(torch.randn(in_channels, out_channels, out_features, in_features))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, in_features, in_channels).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, out_features, out_channels).\n",
    "        \"\"\"\n",
    "        assert x.ndim == 3\n",
    "        assert x.shape[1] == self.in_features\n",
    "        assert x.shape[2] == self.in_channels\n",
    "\n",
    "        # shape (batch_size, in_channels, in_features)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # shape (batch_size, in_channels, 1, in_features, 1)\n",
    "        x = x.view(-1, self.in_channels, 1, self.in_features, 1)\n",
    "\n",
    "        # shape (batch_size, in_channels, out_channels, out_features, 1)\n",
    "        all = self.weights @ x + self.b\n",
    "\n",
    "        # shape (batch_size, out_channels, out_features)\n",
    "        reduced = torch.mean(all, dim=(1, 4), keepdim=False)\n",
    "\n",
    "        # shape (batch_size, out_features, out_channels)\n",
    "        reduced = reduced.permute(0, 2, 1)\n",
    "\n",
    "        return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_samples: int,\n",
    "        shape: tuple[int, ...],\n",
    "        var1: float = 1.0,\n",
    "        var2: float = 0.8,\n",
    "        static: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "        self.var1 = var1\n",
    "        self.var2 = var2\n",
    "        self.static = static\n",
    "\n",
    "        self.labels = torch.randint(0, 2, (num_samples,)).to(torch.float32)\n",
    "\n",
    "        if static:\n",
    "            var = var1 * (self.labels == 0) + var2 * (self.labels == 1)\n",
    "            data = torch.randn(num_samples, *shape)\n",
    "            self.data = torch.swapaxes(data.swapaxes(0, -1) * torch.sqrt(var), 0, -1)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[Tensor, Tensor]:\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.static:\n",
    "            return self.data[idx], label\n",
    "\n",
    "        var = self.var1 if label == 0 else self.var2\n",
    "        data = torch.randn(self.shape) * math.sqrt(var)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "N = 10\n",
    "D = 5\n",
    "data_size = 1000\n",
    "\n",
    "train_size = int(0.8 * data_size)\n",
    "test_size = data_size - train_size\n",
    "\n",
    "ds_train = GaussianDataset(\n",
    "    num_samples=train_size,\n",
    "    shape=(N, D),\n",
    "    var1=1.0,\n",
    "    var2=0.8,\n",
    "    static=False,\n",
    ")\n",
    "\n",
    "ds_test = GaussianDataset(\n",
    "    num_samples=test_size,\n",
    "    shape=(N, D),\n",
    "    var1=1.0,\n",
    "    var2=0.8,\n",
    "    static=True,\n",
    ")\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    dataset=ds_train,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "dl_test = DataLoader(\n",
    "    dataset=ds_test,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import BinaryTrainer\n",
    "\n",
    "# TODO: DOES NOT TRAIN WTF :C\n",
    "# NONE OF THE NETWORKS EXHIBIT ANY FORM OF LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    LinearEquivariant(in_channels=D, out_channels=10),\n",
    "    nn.ReLU(),\n",
    "    LinearEquivariant(in_channels=10, out_channels=10),\n",
    "    nn.ReLU(),\n",
    "    LinearInvariant(in_channels=10, out_channels=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    "    log=True,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=300,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Sequential(\n",
    "    LinearMultiChannel(in_channels=D, out_channels=10, in_features=N, out_features=N),\n",
    "    nn.ReLU(),\n",
    "    LinearMultiChannel(in_channels=10, out_channels=10, in_features=N, out_features=N),\n",
    "    nn.ReLU(),\n",
    "    LinearMultiChannel(in_channels=10, out_channels=1, in_features=N, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "model = CanonicalModel(layers)\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    "    log=True,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    dl_train=dl_train,\n",
    "    dl_test=dl_test,\n",
    "    num_epochs=300,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Sequential(\n",
    "    LinearMultiChannel(in_channels=D, out_channels=10, in_features=N, out_features=N),\n",
    "    nn.ReLU(),\n",
    "    LinearMultiChannel(in_channels=10, out_channels=10, in_features=N, out_features=N),\n",
    "    nn.ReLU(),\n",
    "    LinearMultiChannel(in_channels=10, out_channels=1, in_features=N, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "perms = list(create_permutations_from_generators([Permutation(torch.arange(hidden_dim))]))\n",
    "\n",
    "model = SymmetryModel(layers, perms)\n",
    "\n",
    "trainer = BinaryTrainer(\n",
    "    model=model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "trainer.fit(dl_train=dl_train, dl_test=dl_test, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges encountered during Implementation:\n",
    "\n",
    "### Numeric Errors:\n",
    "\n",
    "The first challenge encountered is in the implementation of the invariant and equivariant layers.\n",
    "The main implementation challenge rose from the fact that in the lecture, the equivariant layer is formulated as follows:\n",
    "\n",
    "$$ F(x) : \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d'} $$\n",
    "\n",
    "$$ F(x)_j = \\sum _{i=1} ^ {d} L_{ij}(x) $$ \n",
    "where $L_{ij}(x)$ is a single feature linear equivariant layer.\n",
    "\n",
    "Technically, this implementation is indeed correct, but the summation over all $L_{ij}(x)$ might causes layer outputs to blow-up.  \n",
    "As result, the outputs of the $F \\circ a \\circ F ...$ become very large.\n",
    "\n",
    "Our network is composed of these layers $\\phi \\circ F \\circ a \\circ F ...$, when $\\phi$ is the sigmoid function that returns values between 0 and 1.\n",
    "\n",
    "Since the last layer of the network is a sigmoid function, and the results of the previous layers are very large (their absolute value), the sigmoid function saturates and returns either 0.0 or 1.0. Because the sigmoid function got saturated, the propagated gradients become 0, hence the network does not learn.\n",
    "\n",
    "To resolve this issue we defined the equivariant layer as follows:\n",
    "\n",
    "$$ F(x)_j = \\frac{1}{d} \\sum _{i=1} ^ {d} L_{ij}(x) $$ \n",
    "\n",
    "This formulation still retains the equivariance property, but it prevents the layer outputs from blowing-up.\n",
    "\n",
    "*Note: We applied the same averaging technique to the invariant layers as well.*\n",
    "\n",
    "### Overfitting:\n",
    "\n",
    "Another big issue we encountered was overfitting. To overcome it, we added an option to dynamically generate the data every time the `Dataset` is accessed. \n",
    "This way, the model never sees the same data twice, and not able to overfit. That indeed resolved completely the overfitting issue.\n",
    "\n",
    "To compare across different dataset sizes, we trained all our models on the same number of epochs with dynamically generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-groups",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

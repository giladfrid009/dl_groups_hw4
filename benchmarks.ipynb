{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gaussian_dataset import GaussianDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "N = 10\n",
    "D = 5\n",
    "data_samples = 1000\n",
    "\n",
    "train_size = int(0.8 * data_samples)\n",
    "test_size = data_samples - train_size\n",
    "\n",
    "ds_train = GaussianDataset(num_samples=train_size, shape=(N, D), var1=1.0, var2=0.8, static=False)\n",
    "\n",
    "ds_test = GaussianDataset(num_samples=test_size, shape=(N, D), var1=1.0, var2=0.8, static=True)\n",
    "\n",
    "dl_train = DataLoader(dataset=ds_train, batch_size=32, shuffle=False)\n",
    "\n",
    "dl_test = DataLoader(dataset=ds_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import test_invariant, test_equivariant\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashlat/miniforge3/envs/dl-groups/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.training import BinaryTrainer\n",
    "from src.layers import LinearEquivariant, LinearInvariant, PositionalEncoding\n",
    "\n",
    "\n",
    "def create_mlp_model(n: int, d: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(start_dim=1),\n",
    "        nn.Linear(in_features=n * d, out_features=10 * d),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=10 * d, out_features=10 * d),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=10 * d, out_features=1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_transformer_model(n: int, d: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        PositionalEncoding(d_model=d, max_len=n),\n",
    "        nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(batch_first=True, d_model=d, nhead=1),\n",
    "            norm=nn.LayerNorm(normalized_shape=d),\n",
    "            num_layers=1,\n",
    "        ),\n",
    "        nn.Flatten(start_dim=1),\n",
    "        nn.Linear(in_features=n * d, out_features=1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_invariant_model(n: int, d: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        LinearEquivariant(in_channels=d, out_channels=10),\n",
    "        nn.ReLU(),\n",
    "        LinearEquivariant(in_channels=10, out_channels=10),\n",
    "        nn.ReLU(),\n",
    "        LinearInvariant(in_channels=10, out_channels=1),\n",
    "        nn.Sigmoid(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'train_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain_results\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FitResult\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model: nn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FitResult:\n\u001b[1;32m      5\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m BinaryTrainer(\n\u001b[1;32m      6\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      7\u001b[0m         criterion\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'train_results'"
     ]
    }
   ],
   "source": [
    "from src.train_results import FitResult\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module) -> FitResult:\n",
    "    trainer = BinaryTrainer(\n",
    "        model=model,\n",
    "        criterion=nn.BCELoss(),\n",
    "        optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "        device=device,\n",
    "        log=True,\n",
    "    )\n",
    "\n",
    "    return trainer.fit(\n",
    "        dl_train=dl_train,\n",
    "        dl_test=dl_test,\n",
    "        num_epochs=10000,\n",
    "        print_every=25,\n",
    "        time_limit=60 * 30,\n",
    "        early_stopping=50,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonization Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import CanonicalModel\n",
    "\n",
    "model = CanonicalModel(create_mlp_model(N, D))\n",
    "\n",
    "test_invariant(model, input=torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CanonicalModel(create_transformer_model(N, D))\n",
    "\n",
    "test_invariant(model, input=torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetrization Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.permutation import Permutation, create_all_permutations, create_permutations_from_generators\n",
    "from src.models import SymmetryModel\n",
    "\n",
    "shift_perm = Permutation((torch.arange(N) + 1) % N)\n",
    "\n",
    "model = SymmetryModel(\n",
    "    model=create_mlp_model(N, D),\n",
    "    perm_creator=lambda: create_permutations_from_generators([shift_perm]),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_perm = Permutation((torch.arange(N) + 1) % N)\n",
    "\n",
    "model = SymmetryModel(\n",
    "    model=create_transformer_model(N, D),\n",
    "    perm_creator=lambda: create_permutations_from_generators([shift_perm]),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP-Based Sampled Symmetrization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = int(math.factorial(N) * 0.05)\n",
    "num = 30\n",
    "\n",
    "model = SymmetryModel(\n",
    "    model=create_mlp_model(N, D),\n",
    "    perm_creator=lambda: (Permutation(torch.randperm(N)) for _ in range(num)),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intrinsic Invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_invariant_model(N, D)\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentation(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Randomly permute the input tensor along the channel dimension.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, d, channel)\n",
    "        \"\"\"\n",
    "        rnd = torch.randn_like(x)\n",
    "        indices = rnd.argsort(dim=-1)\n",
    "        result = torch.gather(x, -1, indices)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Augmentation(),\n",
    "    create_invariant_model(N, D),\n",
    ")\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-groups",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gaussian_dataset import GaussianDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "N = 10\n",
    "D = 5\n",
    "data_samples = 1000\n",
    "\n",
    "train_size = int(0.8 * data_samples)\n",
    "test_size = data_samples - train_size\n",
    "\n",
    "ds_train = GaussianDataset(num_samples=train_size, shape=(N, D), var1=1.0, var2=0.8, static=False)\n",
    "\n",
    "ds_test = GaussianDataset(num_samples=test_size, shape=(N, D), var1=1.0, var2=0.8, static=True)\n",
    "\n",
    "dl_train = DataLoader(dataset=ds_train, batch_size=32, shuffle=False)\n",
    "\n",
    "dl_test = DataLoader(dataset=ds_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import test_invariant, test_equivariant\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training import BinaryTrainer\n",
    "from src.layers import LinearEquivariant, LinearInvariant, PositionalEncoding\n",
    "\n",
    "\n",
    "def create_mlp_model(n: int, d: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(start_dim=1),\n",
    "        nn.Linear(in_features=n * d, out_features=10 * d),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=10 * d, out_features=10 * d),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=10 * d, out_features=1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_transformer_model(n: int, d: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        PositionalEncoding(d_model=d, max_len=n),\n",
    "        nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(batch_first=True, d_model=d, nhead=1),\n",
    "            norm=nn.LayerNorm(normalized_shape=d),\n",
    "            num_layers=1,\n",
    "        ),\n",
    "        nn.Flatten(start_dim=1),\n",
    "        nn.Linear(in_features=n * d, out_features=1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_invariant_model(n: int, d: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        LinearEquivariant(in_channels=d, out_channels=10),\n",
    "        nn.ReLU(),\n",
    "        LinearEquivariant(in_channels=10, out_channels=10),\n",
    "        nn.ReLU(),\n",
    "        LinearInvariant(in_channels=10, out_channels=1),\n",
    "        nn.Sigmoid(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_results import FitResult\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, log_dir: str | None = None) -> FitResult:\n",
    "    trainer = BinaryTrainer(\n",
    "        model=model,\n",
    "        criterion=nn.BCELoss(),\n",
    "        optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "        device=device,\n",
    "        log=True,\n",
    "        log_dir=log_dir,\n",
    "    )\n",
    "\n",
    "    return trainer.fit(\n",
    "        dl_train=dl_train,\n",
    "        dl_test=dl_test,\n",
    "        num_epochs=10000,\n",
    "        print_every=25,\n",
    "        time_limit=60 * 30,\n",
    "        early_stopping=300,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonization Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import CanonicalModel\n",
    "\n",
    "model = CanonicalModel(create_mlp_model(N, D))\n",
    "\n",
    "test_invariant(model, input=torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, log_dir=\"runs/canonical-mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CanonicalModel(create_transformer_model(N, D))\n",
    "\n",
    "test_invariant(model, input=torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, log_dir=\"runs/canonical-attn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetrization Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.permutation import Permutation, create_all_permutations, create_permutations_from_generators\n",
    "from src.models import SymmetryModel\n",
    "\n",
    "shift_perm = Permutation((torch.arange(N) + 1) % N)\n",
    "\n",
    "model = SymmetryModel(\n",
    "    model=create_mlp_model(N, D),\n",
    "    perm_creator=lambda: create_permutations_from_generators([shift_perm]),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(model, log_dir=\"runs/symmetry-mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_perm = Permutation((torch.arange(N) + 1) % N)\n",
    "\n",
    "model = SymmetryModel(\n",
    "    model=create_transformer_model(N, D),\n",
    "    perm_creator=lambda: create_permutations_from_generators([shift_perm]),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(model, log_dir=\"runs/symmetry-mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP-Based Sampled Symmetrization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = int(math.factorial(N) * 0.05)\n",
    "num = 30\n",
    "\n",
    "model = SymmetryModel(\n",
    "    model=create_mlp_model(N, D),\n",
    "    perm_creator=lambda: (Permutation(torch.randperm(N)) for _ in range(num)),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, log_dir=\"runs/symmetry-sampling-mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention-Based Sampled Symmetrization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = int(math.factorial(N) * 0.05)\n",
    "num = 30\n",
    "\n",
    "model = SymmetryModel(\n",
    "    model=create_transformer_model(N, D),\n",
    "    perm_creator=lambda: (Permutation(torch.randperm(N)) for _ in range(num)),\n",
    "    chunksize=10,\n",
    ")\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, log_dir=\"runs/symmetry-sampling-attn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intrinsic Invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_invariant_model(N, D)\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, log_dir=\"runs/intrinsic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmentation(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Randomly permute the input tensor along the channel dimension.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, d, channel)\n",
    "        \"\"\"\n",
    "        rnd = torch.randn_like(x)\n",
    "        indices = rnd.argsort(dim=-1)\n",
    "        result = torch.gather(x, -1, indices)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP-Based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Augmentation(),\n",
    "    create_mlp_model(N, D),\n",
    ")\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, log_dir=\"runs/augmented-mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention-Based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Augmentation(),\n",
    "    create_transformer_model(N, D),\n",
    ")\n",
    "\n",
    "test_invariant(model, torch.randn(32, N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, log_dir=\"runs/augmented-attn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-groups",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
